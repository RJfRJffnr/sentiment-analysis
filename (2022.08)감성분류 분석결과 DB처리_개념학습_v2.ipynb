{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5bea1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_18820\\486238426.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.container { width:100% !important; }\n",
       "div.CodeMirror {font-family: Consolas; font-size: 16pt;}\n",
       "div.output { font-size: 16pt; font-weight: bold;}\n",
       "div.input { font-family: Consolas; font-size: 16pt;}\n",
       "div.prompt { min-width: 100px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\n",
    "\"\"\"<style>\n",
    "div.container { width:100% !important; }\n",
    "div.CodeMirror {font-family: Consolas; font-size: 16pt;}\n",
    "div.output { font-size: 16pt; font-weight: bold;}\n",
    "div.input { font-family: Consolas; font-size: 16pt;}\n",
    "div.prompt { min-width: 100px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a6dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9985fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긍부정 처리\n",
    "\n",
    "import re\n",
    "import pymysql\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "okt = Okt()\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375e0ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 모델 복원\n",
    "loaded_model = tf.keras.models.load_model('best_model.h5')\n",
    "# loaded_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d2a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#### 중요!!!!!!! Tokenizer object loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a904a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "DB_HOST = 'localhost'\n",
    "DB_USER = 'root'\n",
    "DB_PASSWD = 'autoset'\n",
    "DB_NAME = 'python'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e985e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "None\n",
      "\n",
      "\n",
      "처리할 데이터 없음\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "phrase input should be string, not <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m처리할 데이터 없음\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43mokt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 토큰화\u001b[39;00m\n\u001b[0;32m     29\u001b[0m content \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m content \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m stopwords] \u001b[38;5;66;03m# 불용어 제거\u001b[39;00m\n\u001b[0;32m     31\u001b[0m encoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([content]) \u001b[38;5;66;03m# 정수 인코딩\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:89\u001b[0m, in \u001b[0;36mOkt.morphs\u001b[1;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmorphs\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;124;03m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [s \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:69\u001b[0m, in \u001b[0;36mOkt.pos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos\u001b[39m(\u001b[38;5;28mself\u001b[39m, phrase, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    In contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    this POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    :param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mvalidate_phrase_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjki\u001b[38;5;241m.\u001b[39mtokenize(\n\u001b[0;32m     72\u001b[0m                 phrase,\n\u001b[0;32m     73\u001b[0m                 jpype\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mBoolean(norm),\n\u001b[0;32m     74\u001b[0m                 jpype\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mBoolean(stem))\u001b[38;5;241m.\u001b[39mtoArray()\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m join:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_common.py:20\u001b[0m, in \u001b[0;36mvalidate_phrase_inputs\u001b[1;34m(phrase)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"validate if phrase input is provided in str format\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    phrase (str): phrase input\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphrase input should be string, not \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(phrase)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(phrase, basestring), msg\n",
      "\u001b[1;31mAssertionError\u001b[0m: phrase input should be string, not <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "conn = pymysql.connect(host=DB_HOST, user=DB_USER, password=DB_PASSWD,\n",
    "                   db=DB_NAME, charset='utf8')\n",
    "curs = conn.cursor()\n",
    "sql = \"\"\"\n",
    "     select no, content \n",
    "     from nblog\n",
    "     where posneg_grade = 'W'\n",
    "     limit 1;\n",
    " \"\"\"\n",
    "curs.execute(sql)\n",
    "table_data = curs.fetchall() \n",
    "conn.close()\n",
    "\n",
    "table_data = [list(table_data[x]) for x in range(len(table_data))]       # tupple을 list형태로 변경\n",
    "# print( table_data  )     # list \n",
    "\n",
    "no = table_data[0][0]\n",
    "content = table_data[0][1]\n",
    "print(no)\n",
    "print(content)\n",
    "\n",
    "try:   # DB에서 가져올 데이터가 없을 때 처리\n",
    "    content = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', content)\n",
    "    # print(content)  # 불필요한 글자들 정리\n",
    "except:\n",
    "    print('\\n\\n처리할 데이터 없음\\n\\n')\n",
    "\n",
    "content = okt.morphs(content, stem=True) # 토큰화\n",
    "content = [word for word in content if not word in stopwords] # 불용어 제거\n",
    "\n",
    "encoded = tokenizer.texts_to_sequences([content]) # 정수 인코딩\n",
    "pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "score = float(loaded_model.predict(pad_new)) # 예측\n",
    "\n",
    "if(score > 0.5):\n",
    "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    posneg_rate = score * 100\n",
    "    posneg_grade = 'Good'\n",
    "else:\n",
    "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))\n",
    "    posneg_rate = (1 - score) * 100\n",
    "    posneg_grade = 'Bad' \n",
    "\n",
    "# print(posneg_rate)\n",
    "# print(posneg_grade)\n",
    "\n",
    "conn = pymysql.connect(host=DB_HOST, user=DB_USER, password=DB_PASSWD,\n",
    "                   db=DB_NAME, charset='utf8')\n",
    "curs = conn.cursor()\n",
    "sql = \"\"\"\n",
    "     update nblog\n",
    "     set posneg_rate={posneg_rate}, posneg_grade='{posneg_grade}'\n",
    "     where no = {no}\n",
    " \"\"\"\n",
    "sql = sql.format(posneg_rate=posneg_rate, posneg_grade=posneg_grade, no=no )\n",
    "curs.execute(sql)\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7dc10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6288e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966e814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
